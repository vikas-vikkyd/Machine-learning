{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_basic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZsAlfDxH18R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d9593a-c8b6-45b7-ffe0-4779f0ad583b"
      },
      "source": [
        "#Import required library\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "#Get training File\n",
        "dataset_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv'\n",
        "train_data_dir = tf.keras.utils.get_file('bbc-text.csv', origin=dataset_url)\n",
        "train_data = pathlib.Path(train_data_dir)\n",
        "\n",
        "#Data Prep\n",
        "sentences = []\n",
        "labels = []\n",
        "label_dict = {'entertainment': 0, 'sport': 1, 'business': 2, 'politics': 3, 'tech': 4}\n",
        "with open(train_data, 'r') as file:\n",
        "    i = 0\n",
        "    for line in file:\n",
        "        line = line.replace('\\n', '')\n",
        "        if i == 0:\n",
        "            i = i + 1\n",
        "        else:\n",
        "            labels.append(label_dict[line.split(',')[0]])\n",
        "            sentences.append(line.split(',')[1])\n",
        "#Data Preprocessing\n",
        "tokenizer = Tokenizer(oov_token='<oov>',num_words=10000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "#Create_sequencs\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "#Get max length of sequences\n",
        "max_len = 0\n",
        "for seq in sequences:\n",
        "    ln = len(seq)\n",
        "    if ln > max_len:\n",
        "        max_len = ln\n",
        "#Parameter for padding\n",
        "trunc_type = 'post'\n",
        "padding = 'post'\n",
        "#Pad sequences\n",
        "padded = pad_sequences(sequences, padding=padding, truncating=trunc_type, maxlen=max_len)\n",
        "#Define Training data\n",
        "X_train = np.array(padded)\n",
        "y_train = tf.keras.utils.to_categorical(labels, num_classes=5)\n",
        "#Define Model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=10000, output_dim=20, input_length=max_len),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu', input_shape=(4491,)),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Train Model\n",
        "history = model.fit(X_train, y_train, epochs=20, verbose=0)\n",
        "#Get embedding\n",
        "embedding = model.layers[0]\n",
        "emb_weights = embedding.get_weights()[0]\n",
        "#Create Word embedding file\n",
        "index_word = dict((value, key) for (key, value) in tokenizer.word_index.items())\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "for i in range(10000):\n",
        "    word = index_word[i+1]\n",
        "    emb = emb_weights[i]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in emb]) + '\\n')\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "#Predict\n",
        "test_sentence = ['virat played really well and it was a team effort']\n",
        "test_sequence = tokenizer.texts_to_sequences(test_sentence)\n",
        "test_padded = pad_sequences(test_sequence, padding=padding, truncating=trunc_type, maxlen=max_len)\n",
        "dict_label = dict((value, key) for (key, value) in label_dict.items())\n",
        "print(dict_label[np.argmax(model.predict(test_padded))])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv\n",
            "5062656/5057493 [==============================] - 0s 0us/step\n",
            "business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgnNQWULcXOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}