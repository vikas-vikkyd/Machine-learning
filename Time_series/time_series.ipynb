{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "time_series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZsAlfDxH18R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778b6101-4503-47b9-9d65-c2bf40cd278e"
      },
      "source": [
        "#Import required library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "#Get data\n",
        "dataset_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "data_dir = tf.keras.utils.get_file('daily-min-temperatures.csv', origin=dataset_url)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "#read data\n",
        "time = []\n",
        "temperature = []\n",
        "with open(data_dir, 'r') as file:\n",
        "    i = 0\n",
        "    for line in file:\n",
        "        if i == 0:\n",
        "            i = i + 1\n",
        "        else:\n",
        "            time.append(line.split(',')[0])\n",
        "            temperature.append(float(line.split(',')[1]))\n",
        "#Declare Training parameter\n",
        "split_size = 2500\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 10000\n",
        "#Split training and testing data\n",
        "train_tempreature = temperature[:split_size]\n",
        "test_tempreature = temperature[split_size:]\n",
        "#Define function for data preprocessing\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size+1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda  window: window.batch(window_size+1))\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "train_dataset = windowed_dataset(train_tempreature, window_size, batch_size, shuffle_buffer_size)\n",
        "test_dataset = windowed_dataset(test_tempreature, window_size, batch_size, shuffle_buffer_size)\n",
        "#Define model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
        "history = model.fit(train_dataset, validation_data=test_dataset, epochs=30, verbose=0)\n",
        "#Forecast\n",
        "val_tempreature = temperature[split_size-window_size:]\n",
        "def windowed_dataset_forcat(series, window_size, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda  window: window.batch(window_size))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "val_dataset = windowed_dataset_forcat(val_tempreature, window_size, 32)\n",
        "result = np.array(model.predict(val_dataset))[0:-1]\n",
        "forcast = []\n",
        "for dat in result:\n",
        "    forcast.append(dat[0])\n",
        "print(tf.keras.metrics.mse(test_tempreature, forcast).numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.208928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgnNQWULcXOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}